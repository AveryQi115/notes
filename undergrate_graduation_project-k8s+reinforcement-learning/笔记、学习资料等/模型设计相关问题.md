模型设计相关问题

1. 如何通过强化学习模型来解决微服务动态部署问题(以容器为例)？

结合经典的强化学习模型和神经网络，对微服务动态部署问题进行建模。在开源数据集VMAgent上训练Q-learning，DQN，演员评论家模型并评估模型的结果得出适合微服务动态部署问题的模型架构。目前已完成了对比评估，得出结论演员评论家模型更适合微服务动态部署问题。

1. 如何对微服务动态部署问题建模？

在微服务动态部署问题中，模型对于状态，动作，奖励的建模如下所示：

\1) 状态：集群运行状态 

集群运行状态为4个维度的向量，这4个维度分别代表：时间，Node序号，Pod序号和资源类别序号。集群运行状态向量展现了在一段时间内，不同Node上不同Pod的各种资源利用率和变化情况。

\2) 动作：模型做出的部署决策

动作为一个长度为Pod数量的向量，其中，向量的第i个元素代表第i个Pod要迁移的目标Node序号。如果第i个Pod要迁移的目标Node是该Pod当前所在的Node，则代表该动作中第i个Pod不需要迁移。

在这样的建模下，有效的动作空间为一次动作迁移最少0个Pod，最多全部Pod。

\3) 奖励：在当前运行状态下，应用模型做出的部署决策所获得的集群和微服务运行实例在资源考量指标、互补性考量指标和迁移成本考量指标方面的提升

奖励为资源考量惩罚，互补性考量惩罚和迁移成本考量惩罚加权和的相反数。

其中，资源考量惩罚为违反cpu利用率限制和mem利用率限制的次数。

互补性考量惩罚为t-1时刻到t时刻间node的cpu和mem需求量的互补性。

迁移成本考量惩罚正相关于执行迁移操作的Pod镜像大小，负相关于该Pod的剩余作业时间。

1. 在动作空间离散的情况下，如何与模型设计结合计算最优的Q值？

根据演员评论家模型的设计，评论家模型输入环境(状态)，动作，输出Q值(输入环境下做出输入动作的期望累积奖励)；演员模型输入环境(状态)，输出动作，这样的设计分离了最大Q值的计算和动作的计算，不需要遍历动作空间计算最优的Q值。另一方面，设计演员模型先输出形状为(Pod个数，Node个数)的向量，再通过gumbel_softmax计算得出针对每个Pod，最优的迁移目标Node，这样在得到符合建模的动作的同时，使得该步骤可导。

工具开发相关问题

 \1. 监控模块粒度为Pod，执行模块粒度为Deployment，模型表示为向量化，如何开发工具使得决策模块能够在模型池中加载不同的动态部署模型并通过与监控模块，执行模块交互进行连续的迁移决策？

不同的模型对于环境(状态)的表示不同，模型应有配套的输入输出类，在其中存储Deployment名称到序号的双向映射，编码集群状态并构造输入模型样本，解码模型输出为Deployment名称到物理节点名称的映射；监控模块的数据应转化为Deployment粒度，需要保证Deployment和Pod一一映射。



\1. 主动式微服务动态部署模型优化

收集不同请求状态下压测的数据，根据不同的请求状态训练模型，并优化模型的设计

\2. 可视化模块优化

在工具开发方面，优化可视化模块，更直观地展示k8s集群实时的更改和迁移